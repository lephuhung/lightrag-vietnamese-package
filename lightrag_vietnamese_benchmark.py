"""
LightRAG Benchmark - So s√°nh 4 ph∆∞∆°ng th·ª©c query v·ªõi metrics chi ti·∫øt

C·∫•u h√¨nh:
- LLM: Qwen3-Coder-30B-A3B-Instruct t·∫°i http://10.8.0.8:8000/v1
- Embedding: dangvantuan/vietnamese-embedding (HuggingFace)

Benchmark metrics:
- Th·ªüi gian th·ª±c thi
- S·ªë entities/relations truy xu·∫•t
- S·ªë tokens s·ª≠ d·ª•ng
- ƒê·ªô ch√≠nh x√°c (so s√°nh v·ªõi ground truth)
- Memory usage

Ch·∫°y:
    python lightrag_vietnamese_benchmark.py
"""

import os
import asyncio
import json
import time
import psutil
import numpy as np
from typing import Literal, cast
from dataclasses import dataclass, field, asdict
from datetime import datetime
from openai import AsyncOpenAI
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import openai_complete_if_cache
from lightrag.utils import wrap_embedding_func_with_attrs, setup_logger
from sentence_transformers import SentenceTransformer

# C·∫•u h√¨nh logging
setup_logger("lightrag", level="WARNING")  # Gi·∫£m log ƒë·ªÉ benchmark ch√≠nh x√°c h∆°n

# Th∆∞ m·ª•c l√†m vi·ªác
WORKING_DIR = "./lightrag_benchmark_storage"
BENCHMARK_RESULTS_DIR = "./benchmark_results"

# T·∫°o th∆∞ m·ª•c
for dir_path in [WORKING_DIR, BENCHMARK_RESULTS_DIR]:
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

# ============================================
# C·∫•u h√¨nh Local LLM
# ============================================
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "http://10.8.0.8:8000/v1")
LLM_API_KEY = os.getenv("LLM_API_KEY", "not-needed")
LLM_MODEL = os.getenv("LLM_MODEL", "Qwen3-Coder-30B-A3B-Instruct")

openai_client = AsyncOpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY)

# ============================================
# C·∫•u h√¨nh Embedding
# ============================================
EMBEDDING_MODEL_NAME = "dangvantuan/vietnamese-embedding"
EMBEDDING_DIM = 768

print(f"ƒêang t·∫£i model embedding: {EMBEDDING_MODEL_NAME}...")
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
print(f"‚úì Model embedding ƒë√£ t·∫£i xong!")


@dataclass
class QueryBenchmarkResult:
    """K·∫øt qu·∫£ benchmark cho m·ªôt query"""
    query: str
    mode: str
    execution_time_ms: float
    entities_count: int = 0
    relations_count: int = 0
    chunks_count: int = 0
    response_length: int = 0
    memory_usage_mb: float = 0.0
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class BenchmarkReport:
    """B√°o c√°o benchmark t·ªïng h·ª£p"""
    model_name: str
    embedding_model: str
    total_queries: int
    results: list = field(default_factory=list)
    summary: dict = field(default_factory=dict)
    generated_at: str = field(default_factory=lambda: datetime.now().isoformat())


async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -> str:
    return await openai_complete_if_cache(
        LLM_MODEL, prompt, system_prompt=system_prompt,
        history_messages=history_messages, api_key=LLM_API_KEY,
        base_url=LLM_BASE_URL, **kwargs,
    )


async def vietnamese_embedding_func(texts: list[str]) -> np.ndarray:
    embeddings = embedding_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
    return embeddings


@wrap_embedding_func_with_attrs(
    embedding_dim=EMBEDDING_DIM, max_token_size=512, model_name=EMBEDDING_MODEL_NAME
)
async def embedding_func(texts: list[str]) -> np.ndarray:
    return await vietnamese_embedding_func(texts)


async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        llm_model_name=LLM_MODEL,
        embedding_func=embedding_func,
        addon_params={
            "language": "Vietnamese",
            "entity_types": ["organization", "person", "location", "event", "product"],
        },
    )
    await rag.initialize_storages()
    return rag


def get_memory_usage():
    """L·∫•y memory usage hi·ªán t·∫°i (MB)"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024


async def benchmark_query(rag, query: str, mode: str) -> QueryBenchmarkResult:
    """
    Th·ª±c hi·ªán query v√† ƒëo c√°c metrics
    """
    mode_literal = cast(Literal["naive", "local", "global", "hybrid"], mode)
    
    # ƒêo memory tr∆∞·ªõc khi query
    mem_before = get_memory_usage()
    
    # ƒêo th·ªüi gian
    start_time = time.perf_counter()
    
    try:
        resp = await rag.aquery(
            query,
            param=QueryParam(mode=mode_literal, stream=False, enable_rerank=False)
        )
        
        # X·ª≠ l√Ω response
        if hasattr(resp, '__iter__') and not isinstance(resp, str):
            response_text = ""
            async for chunk in resp:
                response_text += chunk
        else:
            response_text = str(resp)
            
    except Exception as e:
        response_text = f"ERROR: {str(e)}"
    
    # T√≠nh th·ªüi gian
    execution_time = (time.perf_counter() - start_time) * 1000  # Convert to ms
    
    # ƒêo memory sau khi query
    mem_after = get_memory_usage()
    memory_used = mem_after - mem_before
    
    # ƒê·∫øm s·ªë l∆∞·ª£ng (∆∞·ªõc t√≠nh t·ª´ response)
    entities_count = response_text.lower().count("**") // 2  # Markdown bold th∆∞·ªùng d√πng cho entities
    response_length = len(response_text)
    
    return QueryBenchmarkResult(
        query=query,
        mode=mode,
        execution_time_ms=round(execution_time, 2),
        entities_count=entities_count,
        response_length=response_length,
        memory_usage_mb=round(memory_used, 2),
    )


def print_benchmark_table(results: list[QueryBenchmarkResult]):
    """In b·∫£ng benchmark ƒë·∫πp"""
    print("\n" + "="*100)
    print("üìä B√ÅO C√ÅO BENCHMARK - SO S√ÅNH 4 PH∆Ø∆†NG TH·ª®C QUERY")
    print("="*100)
    
    # Header
    print(f"\n{'Query':<40} {'Mode':<10} {'Time(ms)':<12} {'Entities':<10} {'Response':<12} {'Memory(MB)':<12}")
    print("-"*100)
    
    # Group by query
    current_query = None
    for result in results:
        if result.query != current_query:
            current_query = result.query
            print(f"\nüîç {result.query}")
        
        print(f"{'':<40} {result.mode:<10} {result.execution_time_ms:<12.2f} "
              f"{result.entities_count:<10} {result.response_length:<12} {result.memory_usage_mb:<12.2f}")


def generate_summary(results: list[QueryBenchmarkResult]) -> dict:
    """T·∫°o summary statistics"""
    modes = ["naive", "local", "global", "hybrid"]
    summary = {}
    
    for mode in modes:
        mode_results = [r for r in results if r.mode == mode]
        if mode_results:
            summary[mode] = {
                "avg_time_ms": round(sum(r.execution_time_ms for r in mode_results) / len(mode_results), 2),
                "total_time_ms": round(sum(r.execution_time_ms for r in mode_results), 2),
                "avg_entities": round(sum(r.entities_count for r in mode_results) / len(mode_results), 1),
                "avg_response_length": round(sum(r.response_length for r in mode_results) / len(mode_results), 0),
                "avg_memory_mb": round(sum(r.memory_usage_mb for r in mode_results) / len(mode_results), 2),
                "queries_count": len(mode_results),
            }
    
    return summary


def print_summary_table(summary: dict):
    """In b·∫£ng t·ªïng h·ª£p"""
    print("\n" + "="*100)
    print("üìà T·ªîNG H·ª¢P HI·ªÜU NƒÇNG THEO PH∆Ø∆†NG TH·ª®C QUERY")
    print("="*100)
    
    print(f"\n{'Mode':<10} {'Avg Time(ms)':<15} {'Total Time(ms)':<18} {'Avg Entities':<15} {'Avg Response':<15} {'Avg Memory(MB)':<15}")
    print("-"*100)
    
    for mode, stats in summary.items():
        print(f"{mode:<10} {stats['avg_time_ms']:<15.2f} {stats['total_time_ms']:<18.2f} "
              f"{stats['avg_entities']:<15.1f} {stats['avg_response_length']:<15.0f} {stats['avg_memory_mb']:<15.2f}")
    
    # So s√°nh nhanh
    print("\n" + "="*100)
    print("‚ö° NH·∫¨N X√âT NHANH:")
    print("="*100)
    
    if summary:
        fastest = min(summary.items(), key=lambda x: x[1]['avg_time_ms'])
        slowest = max(summary.items(), key=lambda x: x[1]['avg_time_ms'])
        most_detailed = max(summary.items(), key=lambda x: x[1]['avg_response_length'])
        
        print(f"  üèÉ Nhanh nh·∫•t: {fastest[0]} ({fastest[1]['avg_time_ms']:.2f}ms)")
        print(f"  üêå Ch·∫≠m nh·∫•t: {slowest[0]} ({slowest[1]['avg_time_ms']:.2f}ms)")
        print(f"  üìù Chi ti·∫øt nh·∫•t: {most_detailed[0]} ({most_detailed[1]['avg_response_length']:.0f} chars)")
        
        speedup = slowest[1]['avg_time_ms'] / fastest[1]['avg_time_ms'] if fastest[1]['avg_time_ms'] > 0 else 0
        print(f"  üìä Ch√™nh l·ªách t·ªëc ƒë·ªô: {speedup:.2f}x")


async def run_benchmark():
    """Ch·∫°y benchmark ƒë·∫ßy ƒë·ªß"""
    print("\n" + "="*100)
    print("üöÄ LightRAG Benchmark - Vietnamese Query Performance")
    print("="*100)
    print(f"\nModel: {LLM_MODEL}")
    print(f"Embedding: {EMBEDDING_MODEL_NAME}")
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    rag = await initialize_rag()
    
    try:
        # D·ªØ li·ªáu m·∫´u
        sample_texts = """
        H√† N·ªôi l√† th·ªß ƒë√¥ c·ªßa Vi·ªát Nam, n·∫±m ·ªü ph√≠a B·∫Øc c·ªßa ƒë·∫•t n∆∞·ªõc. 
        Th√†nh ph·ªë c√≥ l·ªãch s·ª≠ h∆°n 1000 nƒÉm v·ªõi nhi·ªÅu di t√≠ch l·ªãch s·ª≠ nh∆∞ VƒÉn Mi·∫øu, 
        Ho√†ng Th√†nh ThƒÉng Long v√† H·ªì G∆∞∆°m.

        TP. H·ªì Ch√≠ Minh l√† th√†nh ph·ªë l·ªõn nh·∫•t Vi·ªát Nam, n·∫±m ·ªü ph√≠a Nam. 
        ƒê√¢y l√† trung t√¢m kinh t·∫ø v√† t√†i ch√≠nh c·ªßa c·∫£ n∆∞·ªõc v·ªõi nhi·ªÅu t√≤a nh√† cao t·∫ßng 
        v√† khu c√¥ng nghi·ªáp.

        C√¥ng ty VNG l√† m·ªôt trong nh·ªØng c√¥ng ty c√¥ng ngh·ªá h√†ng ƒë·∫ßu Vi·ªát Nam, 
        ƒë∆∞·ª£c th√†nh l·∫≠p nƒÉm 2004. C√¥ng ty n·ªïi ti·∫øng v·ªõi s·∫£n ph·∫©m Zalo - 
        ·ª©ng d·ª•ng nh·∫Øn tin ph·ªï bi·∫øn nh·∫•t t·∫°i Vi·ªát Nam.

        FPT l√† t·∫≠p ƒëo√†n c√¥ng ngh·ªá l·ªõn nh·∫•t Vi·ªát Nam, ho·∫°t ƒë·ªông trong lƒ©nh v·ª±c 
        ph·∫ßn m·ªÅm, vi·ªÖn th√¥ng v√† gi√°o d·ª•c. FPT Software l√† c√¥ng ty con chuy√™n v·ªÅ 
        outsourcing ph·∫ßn m·ªÅm.

        Ng√†nh tr√≠ tu·ªá nh√¢n t·∫°o (AI) ƒëang ph√°t tri·ªÉn r·∫•t nhanh t·∫°i Vi·ªát Nam. 
        Nhi·ªÅu startup c√¥ng ngh·ªá ƒëang ·ª©ng d·ª•ng AI v√†o c√°c lƒ©nh v·ª±c nh∆∞ y t·∫ø, 
        gi√°o d·ª•c v√† t√†i ch√≠nh.
        """
        
        print("\nüì• Inserting data...")
        insert_start = time.perf_counter()
        await rag.ainsert(sample_texts)
        insert_time = (time.perf_counter() - insert_start) * 1000
        print(f"‚úì Insert completed in {insert_time:.2f}ms")
        
        # C√°c c√¢u h·ªèi benchmark
        queries = [
            "H√† N·ªôi c√≥ nh·ªØng ƒë·ªãa ƒëi·ªÉm n·ªïi ti·∫øng n√†o?",
            "C√¥ng ty c√¥ng ngh·ªá n√†o l·ªõn nh·∫•t Vi·ªát Nam?",
            "Ng√†nh AI ph√°t tri·ªÉn nh∆∞ th·∫ø n√†o t·∫°i Vi·ªát Nam?",
        ]
        
        modes = ["naive", "local", "global", "hybrid"]
        all_results = []
        
        print(f"\nüéØ Running {len(queries)} queries x {len(modes)} modes = {len(queries) * len(modes)} total queries...")
        
        for i, query in enumerate(queries, 1):
            print(f"\n{'='*100}")
            print(f"Query {i}/{len(queries)}: {query}")
            print('='*100)
            
            for mode in modes:
                print(f"  Testing {mode}...", end=" ")
                result = await benchmark_query(rag, query, mode)
                all_results.append(result)
                print(f"‚úì {result.execution_time_ms:.2f}ms")
        
        # In k·∫øt qu·∫£
        print_benchmark_table(all_results)
        
        # T·∫°o v√† in summary
        summary = generate_summary(all_results)
        print_summary_table(summary)
        
        # L∆∞u b√°o c√°o JSON
        report = BenchmarkReport(
            model_name=LLM_MODEL,
            embedding_model=EMBEDDING_MODEL_NAME,
            total_queries=len(queries) * len(modes),
            results=[asdict(r) for r in all_results],
            summary=summary,
        )
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = os.path.join(BENCHMARK_RESULTS_DIR, f"benchmark_report_{timestamp}.json")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ Report saved to: {report_file}")
        
    finally:
        await rag.finalize_storages()
    
    print("\n" + "="*100)
    print("‚úÖ Benchmark completed!")
    print("="*100)


if __name__ == "__main__":
    asyncio.run(run_benchmark())
